{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso \n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from adjustText import adjust_text\n",
    "from sklearn.dummy import DummyRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/cleaned_with_sentiment_scores_gt20_fuzzy.csv').convert_dtypes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "    'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "    'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "    'review_scores_value', 'median_sentiment_clean', 'average_sentiment_clean',\n",
    "    'median_sentiment_accurate', 'average_sentiment_accurate', 'median_sentiment_checkin',\n",
    "    'average_sentiment_checkin', 'median_sentiment_communication', 'average_sentiment_communication',\n",
    "    'median_sentiment_location', 'average_sentiment_location', 'median_sentiment_value',\n",
    "    'average_sentiment_value', 'median_sentiment_overall', 'average_sentiment_overall'\n",
    "]\n",
    "df = df[(df[columns_to_check] != 0).all(axis=1)]\n",
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_columns = df.iloc[0].isna()\n",
    "\n",
    "na_columns[na_columns].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['review_scores_rating'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(1, 1,1)\n",
    "# plt.scatter(df[f'median_sentiment_overall'], df['review_scores_rating'], alpha=0.5)\n",
    "# plt.title('Mean Overall Sentiment vs Overall Rating (>4)')\n",
    "# plt.xlabel('Mean Sentiment')\n",
    "# plt.ylabel('Overall Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "models = {\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"DummyRegressor\": DummyRegressor()\n",
    "}\n",
    "\n",
    "categories = [\n",
    "    ('overall', 'review_scores_rating'),\n",
    "    ('accurate', 'review_scores_accuracy'),\n",
    "    ('clean', 'review_scores_cleanliness'),\n",
    "    ('checkin', 'review_scores_checkin'),\n",
    "    ('communication', 'review_scores_communication'),\n",
    "    ('location', 'review_scores_location'),\n",
    "    ('value', 'review_scores_value')\n",
    "]\n",
    "scoring = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "# Define hyperparameters\n",
    "param_grids = {\n",
    "    \"Ridge\": {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"DummyRegressor\":{\n",
    "        'strategy': ['mean','median'],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation and evaluate each model\n",
    "results = []\n",
    "for min_reviews in [3,10,20]:\n",
    "    df_test= df[df['review_count'] > min_reviews]\n",
    "    for category in categories:\n",
    "        X = df_test[[f'average_sentiment_{category[0]}', f'median_sentiment_{category[0]}']]\n",
    "        y = df_test[f'{category[1]}']\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "        for name, model in models.items():\n",
    "                print(f\"Training {name}...\")\n",
    "                grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name],scoring=scoring,cv=5, n_jobs=-1, verbose=0)\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                \n",
    "                best_params = grid_search.best_params_\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_pred = best_model.predict(X_test)\n",
    "                \n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': name,\n",
    "                    'Best Params': best_params,\n",
    "                    'MSE': mse,\n",
    "                    'MAE': mae,\n",
    "                    'R^2': r2,\n",
    "                    'Category': category,\n",
    "                    'min_reviews':min_reviews\n",
    "                })\n",
    "            \n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('multi_model_evaluation_all_categories_cv5.csv');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./multi_model_evaluation_all_categories.csv')\n",
    "df2 = pd.read_csv('./multi_model_evaluation_all_categories_dummy.csv')\n",
    "df = pd.concat([df,df2],ignore_index=True).reset_index()\n",
    "df.drop(columns=['index'],inplace=True)\n",
    "rx = re.compile(r\"[^A-Za-z0-9\\s]\")\n",
    "df[['Category']] = df[['Category']].map(lambda x: rx.sub('',x).split()[0])\n",
    "\n",
    "df.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['Model'] == 'DummyRegressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['median_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = df['Category'].unique().tolist()\n",
    "bar_width = 0.2  # Adjusted width for bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plotting\n",
    "for i, min_reviews in enumerate([3, 10, 20]):\n",
    "    mse_values = []\n",
    "    models = []\n",
    "    for category in category_labels:\n",
    "        cat_data = df[(df['Category'] == category) & (df['min_reviews'] == min_reviews)]\n",
    "        min_mse = cat_data.loc[cat_data['R^2'].idxmax()]\n",
    "        mse_values.append(min_mse['R^2'])\n",
    "        models.append(min_mse['Model'])\n",
    "\n",
    "\n",
    "    # Create custom positions for bars to increase spacing\n",
    "    bar_positions = [j + i * bar_width for j in range(len(category_labels))]\n",
    "    \n",
    "    bars = ax.bar(bar_positions, mse_values, width=bar_width, label=f'min_reviews = {min_reviews}', alpha=.7)\n",
    "\n",
    "    # Annotate the bars with model names and MSE values\n",
    "    for bar, model, mse_value in zip(bars, models, mse_values):\n",
    "        if model is not None:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 3,\n",
    "                    f'{model} ({mse_value:.4f})', ha='center', fontsize=8,rotation=90)\n",
    "\n",
    "# Adjust x-ticks to be in the center of the grouped bars\n",
    "center_positions = [j for j in range(len(category_labels))]\n",
    "ax.set_xticks(center_positions)\n",
    "ax.set_xticklabels(category_labels)\n",
    "\n",
    "ax.grid(True, which='Major', linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n",
    "ax.set_xlabel('Category',fontsize=13)\n",
    "ax.set_ylabel('R^2',fontsize=13)\n",
    "ax.legend(title='Min Reviews')\n",
    "ax.set_title('R^2 by Category and Min Reviews Group for dummy model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = df['Category'].unique().tolist()\n",
    "bar_width = 0.2  # Adjusted width for bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plotting\n",
    "for i, min_reviews in enumerate([3, 10, 20]):\n",
    "    mse_values = []\n",
    "    models = []\n",
    "    for category in category_labels:\n",
    "        \n",
    "\n",
    "\n",
    "    # Create custom positions for bars to increase spacing\n",
    "    bar_positions = [j + i * bar_width for j in range(len(category_labels))]\n",
    "    \n",
    "    bars = ax.bar(bar_positions, mse_values, width=bar_width, label=f'min_reviews = {min_reviews}', alpha=.7)\n",
    "\n",
    "    # Annotate the bars with model names and MSE values\n",
    "    for bar, model, mse_value in zip(bars, models, mse_values):\n",
    "        if model is not None:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 3,\n",
    "                    f'{model} ({mse_value:.4f})', ha='center', fontsize=8,rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = df['review_scores_rating'].dropna()\n",
    "dummy = df['median_sentiment_overall'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = dummy.apply(lambda x: x * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((accuracy - dummy)** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
